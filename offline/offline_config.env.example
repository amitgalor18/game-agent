# Copy to offline_config.env and edit paths for your airgapped setup.
# This file is sourced by run_all.sh and install_offline.sh (when present).

# ----- LLM (llama.cpp) -----
# Path to the GGUF model file (relative to bundle root or absolute).
# Change this when you switch to a different LLM model.
LLM_GGUF_PATH=offline/vendor/llm_models/qwen2.5-3b-instruct-q4_k_m.gguf

# Model name as exposed by the OpenAI-compatible API (must match what the backend expects).
# Change this when you switch LLM model (e.g. to match the new model's name in the API).
LM_STUDIO_MODEL=qwen2.5-3b-instruct

# Port for the LLM server (backend expects http://localhost:1234/v1 by default).
LLM_SERVER_PORT=1234

# Optional: chat format for llama-cpp-python. Leave empty for server default.
# Note: "chatml" can make tool calling worse (model may output create_target(Village) in content).
# LLM_CHAT_FORMAT=

# ----- Whisper (ASR) -----
# faster-whisper model name. Change when you switch Whisper model.
# Options: tiny, base, small, medium, large-v1, large-v2, large-v3, large-v3-turbo, distil-large-v3, etc.
WHISPER_MODEL=large-v3-turbo

# Device: cpu (default for airgapped) or cuda
WHISPER_DEVICE=cpu

# ----- Backend / Frontend -----
BACKEND_PORT=8000
FRONTEND_PORT=5173
